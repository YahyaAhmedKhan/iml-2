{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd   \n",
    "import time\n",
    "from sklearn.model_selection import train_test_split #, RepeatedKFold, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "#change working directory to the location of the data file\n",
    "os.chdir('/mnt/d/Sajjad/08-2023/Python Code/Introduction to Machine Learning/')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting data from UCI Machine Learning Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "cdc_diabetes_health_indicators = fetch_ucirepo(id=891) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = cdc_diabetes_health_indicators.data.features \n",
    "y = cdc_diabetes_health_indicators.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(cdc_diabetes_health_indicators.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(cdc_diabetes_health_indicators.variables) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Education and Income are in ordinal scale, hence, no need to do one-hot encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set Education Variable as Categorical\n",
    "#X['Education'] = X['Education'].astype('category')\n",
    "#X['Education'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making two train/test dataset. One without scaling and the other with scaling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without scaling\n",
    "trainX2, testX2, trainy2, testy2 = train_test_split(X, y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with scaling dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaled X\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "trainX, testX, trainy, testy = train_test_split(X_scaled, y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the distribution of y in train and test\n",
    "print(trainy.value_counts())\n",
    "testy.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this type casting is not always required but at times torch generates an error so just as a matter of caution converting all types to float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert all int64 to float32\n",
    "trainX = trainX.astype('float32')\n",
    "testX = testX.astype('float32')\n",
    "trainy = trainy.astype('float32')\n",
    "testy = testy.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using \"torch\" as keras_backend. Could have used tensorflow or jax as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explore batch size, iteration size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of input features in X and assign to n_features\n",
    "n_features = trainX.shape[1]\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer with 10 neurons and specify the input shape\n",
    "model.add(Dense(10, input_dim=n_features, activation='relu'))\n",
    "\n",
    "# Add the second hidden layer with 5 neurons\n",
    "model.add(Dense(5, activation='relu'))\n",
    "\n",
    "# Add the output layer with 1 neuron (for binary classification) and 'sigmoid' activation\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC']) #change optimizer\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(trainX, trainy, epochs=20, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(testX, testy, batch_size=4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparing the performance with catboost, xgboost, and lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_models = 100\n",
    "depth_level = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use lgboost\n",
    "lgb_model = lgb.LGBMClassifier(max_depth=depth_level, n_estimators=num_of_models, learning_rate=0.1)\n",
    "start_time = time.time()\n",
    "#fit xgb_model\n",
    "lgb_model.fit(trainX2,trainy2)\n",
    "md_probs = lgb_model.predict_proba(testX2)\n",
    "md_probs = md_probs[:,1]\n",
    "md_auc = roc_auc_score(testy2, md_probs)\n",
    "print(\"LG Boost\", \" : \", md_auc)\n",
    "#record the end time\n",
    "end_time = time.time()\n",
    "#calculate the total time\n",
    "total_time = end_time - start_time\n",
    "print(\"Total time LGB: \", total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = CatBoostClassifier(iterations=num_of_models, depth=depth_level, learning_rate=0.1, loss_function='Logloss', verbose=False)\n",
    "#record the start time\n",
    "start_time = time.time()\n",
    "cb.fit(trainX2,trainy2)\n",
    "md_probs = cb.predict_proba(testX2)\n",
    "md_probs = md_probs[:,1]\n",
    "md_auc = roc_auc_score(testy2, md_probs)\n",
    "print(\"Cat Boost\", \" : \", md_auc)\n",
    "#record the end time\n",
    "end_time = time.time()\n",
    "#calculate the total time\n",
    "total_time = end_time - start_time\n",
    "print(\"Total time CB: \", total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use xgboost\n",
    "xgb_model = xgb.XGBClassifier(max_depth=depth_level, n_estimators=num_of_models, learning_rate=0.1)\n",
    "start_time = time.time()\n",
    "#fit xgb_model\n",
    "xgb_model.fit(trainX2,trainy2)\n",
    "md_probs = xgb_model.predict_proba(testX2)\n",
    "md_probs = md_probs[:,1]\n",
    "md_auc = roc_auc_score(testy2, md_probs)\n",
    "print(\"XG Boost\", \" : \", md_auc)\n",
    "#record the end time\n",
    "end_time = time.time()\n",
    "#calculate the total time\n",
    "total_time = end_time - start_time\n",
    "print(\"Total time XGB: \", total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using sklearn version of feedforward neural network (also called Multi-layer Perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using multilayer perceptron of sklearn\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10, 5), max_iter=10, solver='adam', verbose=10, random_state=1,\n",
    "                    learning_rate_init=.1, batch_size=2048)\n",
    "#mlp.fit(trainX, trainy)\n",
    "for epoch in range(10):  # Set the desired number of epochs\n",
    "    mlp.partial_fit(trainX, trainy, classes=[0, 1])\n",
    "\n",
    "    # Evaluate on the validation set and print AUC ROC\n",
    "    y_prob = mlp.predict_proba(testX)[:, 1]\n",
    "    auc_roc = roc_auc_score(testy, y_prob)\n",
    "    print(f\"Epoch {epoch + 1}, AUC ROC: {auc_roc:.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experimenting with different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of input features in X and assign to n_features\n",
    "n_features = trainX.shape[1]\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer with 10 neurons and specify the input shape\n",
    "model.add(Dense(20, input_dim=n_features, activation='relu'))\n",
    "\n",
    "# Add the second hidden layer with 5 neurons\n",
    "model.add(Dense(10, activation='relu'))\n",
    "\n",
    "# Add the output layer with 1 neuron (for binary classification) and 'sigmoid' activation\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Create an SGD optimizer with momentum\n",
    "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "# Compile the model\n",
    "#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC']) #change optimizer\n",
    "model.compile(optimizer=sgd_optimizer, loss='binary_crossentropy', metrics=['AUC'])\n",
    "\n",
    "model.fit(trainX, trainy, epochs=20, batch_size=2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(testX, testy, verbose=0, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experimenting with early stopping without validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define EarlyStopping callback to monitor training loss\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with dropout and early stopping\n",
    "model.fit(trainX, trainy, epochs=10, batch_size=2048,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(testX, testy, verbose=0, batch_size=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experimenting with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model (example architecture with dropout)\n",
    "model.add(Dense(20, input_dim=21, activation='relu'))\n",
    "model.add(Dropout(0.25))  # Example dropout layer with a dropout rate of 0.5\n",
    "model.add(Dense(15, activation='relu'))\n",
    "model.add(Dropout(0.25))  # Example dropout layer with a dropout rate of 0.3\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dropout(0.25))  # Example dropout layer with a dropout rate of 0.3\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Create an SGD optimizer with momentum\n",
    "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC']) #change optimizer\n",
    "model.fit(trainX, trainy, epochs=30, batch_size=2048, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model (example architecture with dropout)\n",
    "model.add(Dense(20, input_dim=21, activation='relu'))\n",
    "model.add(Dropout(0.25))  # Example dropout layer with a dropout rate of 0.5\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Create an SGD optimizer with momentum\n",
    "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC']) #change optimizer\n",
    "model.fit(trainX, trainy, epochs=30, batch_size=2048, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experimenting with weights regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model (example architecture with dropout)\n",
    "model.add(Dense(20, input_dim=21, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.25))  # Example dropout layer with a dropout rate of 0.5\n",
    "model.add(Dense(10, input_dim=21, activation='relu', kernel_regularizer=l1(0.01)))\n",
    "model.add(Dropout(0.25))  # Example dropout layer with a dropout rate of 0.5\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Create an SGD optimizer with momentum\n",
    "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC']) #change optimizer\n",
    "model.fit(trainX, trainy, epochs=150, batch_size=2048, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experimenting with early stopping with validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valX, trainX3, valy, trainy3 = train_test_split(trainX, trainy, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model (example architecture with dropout)\n",
    "model.add(Dense(20, input_dim=21, activation='relu'))\n",
    "model.add(Dropout(0.25))  # Example dropout layer with a dropout rate of 0.5\n",
    "model.add(Dense(10, input_dim=21, activation='relu'))\n",
    "model.add(Dropout(0.25))  # Example dropout layer with a dropout rate of 0.5\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Create an SGD optimizer with momentum\n",
    "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC']) #change optimizer\n",
    "model.fit(trainX3, trainy3, validation_data=(valX, valy), epochs=50, batch_size=2048, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experimenting with weights initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import he_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model (example architecture with dropout)\n",
    "model.add(Dense(20, input_dim=21, activation='relu', kernel_initializer=he_normal()))\n",
    "model.add(Dropout(0.25))  # Example dropout layer with a dropout rate of 0.5\n",
    "model.add(Dense(10, input_dim=21, activation='relu'))\n",
    "model.add(Dropout(0.25))  # Example dropout layer with a dropout rate of 0.5\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Create an SGD optimizer with momentum\n",
    "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC']) #change optimizer\n",
    "model.fit(trainX, trainy, validation_data=(valX, valy), epochs=50, batch_size=2048, callbacks=[early_stopping])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e9236c168fd9c5a2497735f30867c8b2a4981b493523b82ffb7e802a066bea3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
