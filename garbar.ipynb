{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import f_regression\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2  # Add this line\n",
    "import tensorflow as tf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Function to create and train a neural network model\n",
    "def create_and_train_nn(X_train, y_train, X_val, y_val, input_dim, hidden_layers, neurons_per_layer, optimizer='adam',\n",
    "                        dropout_rate=0.0, epochs=100, batch_size=32, early_stopping=False, regularization=0.01):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons_per_layer, input_dim=input_dim, activation='relu', kernel_regularizer=l2(regularization)))\n",
    "\n",
    "    # Adding hidden layers with regularization\n",
    "    for i in range(1, hidden_layers):\n",
    "        model.add(Dense(neurons_per_layer, activation='relu', kernel_regularizer=l2(regularization)))\n",
    "        if dropout_rate > 0.0:\n",
    "            model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "    # Define callbacks\n",
    "    callbacks = []\n",
    "    if early_stopping:\n",
    "        callbacks.append(EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True))\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val),\n",
    "                        callbacks=callbacks, verbose=2)\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_nn(X_train, y_train, X_val, y_val, X_test, input_dim, hidden_layers, neurons_per_layer,\n",
    "                           optimizer='adam', dropout_rate=0.0, epochs=100, batch_size=32, early_stopping=False,\n",
    "                           regularization=0.01):\n",
    "    # Create and train the neural network model\n",
    "    model = create_and_train_nn(X_train, y_train, X_val, y_val, input_dim, hidden_layers, neurons_per_layer,\n",
    "                                 optimizer, dropout_rate, epochs, batch_size, early_stopping, regularization)\n",
    "\n",
    "    # Make predictions on the training set for evaluation\n",
    "    pred_train = model.predict(X_train).flatten()\n",
    "\n",
    "    # Print RMSE on the training set for evaluation\n",
    "    rmse_train = sqrt(mean_squared_error(y_train, pred_train))\n",
    "    print(f\"RMSE on Training Set: {rmse_train}\")\n",
    "\n",
    "    # Generate predictions for the validation set\n",
    "    pred_val = model.predict(X_val).flatten()\n",
    "\n",
    "    # Print RMSE on the validation set for evaluation\n",
    "    rmse_val = sqrt(mean_squared_error(y_val, pred_val))\n",
    "    print(f\"RMSE on Validation Set: {rmse_val}\")\n",
    "\n",
    "    # Generate predictions for the test set\n",
    "    pred_test = model.predict(X_test).flatten()\n",
    "\n",
    "    return pred_test, rmse_train, rmse_val\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "df1 = pd.read_csv('C:\\\\Users\\\\DELL\\\\Desktop\\\\Kaggle Competition 2\\\\Data\\\\train.csv')\n",
    "df2 = pd.read_csv('C:\\\\Users\\\\DELL\\\\Desktop\\\\Kaggle Competition 2\\\\Data\\\\test.csv')\n",
    "\n",
    "# Save row IDs for final output\n",
    "row_ids = df2['row ID']\n",
    "\n",
    "# Drop 'sub_area' from both datasets\n",
    "df1 = df1.drop(columns=['sub_area'])\n",
    "df2 = df2.drop(columns=['sub_area', 'row ID'])\n",
    "\n",
    "# Identify and label encode categorical columns\n",
    "categorical_columns = df1.select_dtypes(include='object').columns\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for column in categorical_columns:\n",
    "    df1[column] = label_encoder.fit_transform(df1[column])\n",
    "    df2[column] = label_encoder.transform(df2[column])\n",
    "\n",
    "# Encode categorical variables\n",
    "df1_encoded = pd.get_dummies(df1, drop_first=True)\n",
    "df2_encoded = pd.get_dummies(df2, drop_first=True)\n",
    "\n",
    "# Separate features and target variable for validation set\n",
    "X_val = df1_encoded.drop(columns=['price_doc'])\n",
    "y_val = df1_encoded['price_doc']\n",
    "\n",
    "# Preprocess validation set\n",
    "X_val_processed = preprocess_data(X_val, imputer, scaler, selector, pca, poly_features)\n",
    "\n",
    "# Perform F-test to get p-values for validation set\n",
    "f_values_val, p_values_val = f_regression(X_val_processed, y_val)\n",
    "\n",
    "# Filter features based on p-values for validation set\n",
    "significant_features_val = p_values_val < 0.05\n",
    "X_val_filtered = X_val_processed[:, significant_features_val]\n",
    "\n",
    "# Apply the same preprocessing to the test set\n",
    "X_test_processed = preprocess_data(df2_encoded, imputer, scaler, selector, pca, poly_features)\n",
    "\n",
    "# Neural Network parameters\n",
    "input_dim = X_train_filtered.shape[1]\n",
    "hidden_layers = 2\n",
    "neurons_per_layer = 60 # Adjust as needed\n",
    "optimizer = 'adam'\n",
    "dropout_rate = 0.2\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "early_stopping = True\n",
    "regularization = 0.01  # Adjust the regularization strength as needed\n",
    "\n",
    "predictions_nn, rmse_train_nn, rmse_val_nn = train_and_evaluate_nn(\n",
    "    X_train_filtered, y, X_val_filtered, y_val, X_test_filtered,\n",
    "    input_dim, hidden_layers, neurons_per_layer,\n",
    "    optimizer, dropout_rate, epochs, batch_size,\n",
    "    early_stopping, regularization\n",
    ")\n",
    "\n",
    "\n",
    "# Print neural network details including regularization\n",
    "print(f\"\\nNeural Network Details:\")\n",
    "print(f\"Number of Features Used: {input_dim}\")\n",
    "print(f\"Architecture Details: {hidden_layers} Hidden Layer(s) with {neurons_per_layer} Neurons in each layer\")\n",
    "print(f\"Name of Optimizer: {optimizer}\")\n",
    "print(f\"Dropout Rate: {dropout_rate}\")\n",
    "print(f\"Batch Size: {batch_size}\")\n",
    "print(f\"Number of Epochs: {epochs}\")\n",
    "print(f\"Regularization Strength: {regularization}\")\n",
    "\n",
    "# Print the best model information\n",
    "print(f\"\\nBest Model (Neural Network)\")\n",
    "print(f\"RMSE on Training Set: {rmse_train_nn}\")\n",
    "print(f\"RMSE on Validation Set: {rmse_val_nn}\")\n",
    "\n",
    "# Create a DataFrame with predictions and 'row_ids'\n",
    "df_output_nn = pd.DataFrame({'row ID': row_ids, 'price_doc': predictions_nn})\n",
    "\n",
    "df_output_nn.to_csv('C:\\\\Users\\\\DELL\\\\Desktop\\\\Kaggle Competition 2\\\\Submissions\\\\Entry28.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
