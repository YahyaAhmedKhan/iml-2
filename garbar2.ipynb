{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_neural_network(input_dim):\n",
    "    model = Sequential([\n",
    "        Dropout(0.08),  # Adjusted dropout rate\n",
    "        Dense(512, activation='relu'),  # Increased neurons\n",
    "        Dense(256, activation='relu', input_dim=input_dim),  # Changed neurons\n",
    "        Dense(128, activation='relu'),  # Additional layer\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='linear')  # Output layer\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=0.001)  # Adjusted learning rate\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "# Save row IDs for the final output\n",
    "row_ids = df_test['row ID']\n",
    "\n",
    "# Drop 'sub_area' from both datasets\n",
    "df_train = df_train.drop(columns=['sub_area'])\n",
    "df_test = df_test.drop(columns=['sub_area', 'row ID'])\n",
    "\n",
    "# Identify and label encode categorical columns\n",
    "categorical_columns = df_train.select_dtypes(include='object').columns\n",
    "label_encoder = LabelEncoder()\n",
    "for column in categorical_columns:\n",
    "    df_train[column] = label_encoder.fit_transform(df_train[column])\n",
    "    df_test[column] = label_encoder.transform(df_test[column])\n",
    "\n",
    "# Encode categorical variables\n",
    "X_train = pd.get_dummies(df_train, drop_first=True)\n",
    "X_test = pd.get_dummies(df_test, drop_first=True)\n",
    "\n",
    "# Separate features and target variable\n",
    "features = X_train.drop(columns=['price_doc'])\n",
    "target = X_train['price_doc']\n",
    "\n",
    "# Impute missing values with the median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train = imputer.fit_transform(features)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Scale features to a specific range\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Feature selection based on variance threshold\n",
    "variance_threshold = 0.01  # Set your desired threshold\n",
    "selector = VarianceThreshold(threshold=variance_threshold)\n",
    "X_train_high_variance = selector.fit_transform(X_train_scaled)\n",
    "X_test_high_variance = selector.transform(X_test_scaled)\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "pca_components = 10 # Adjust based on your preference or use model evaluation\n",
    "pca = PCA(n_components=pca_components)\n",
    "X_train_pca = pca.fit_transform(X_train_high_variance)\n",
    "X_test_pca = pca.transform(X_test_high_variance)\n",
    "\n",
    "# Create polynomial features without interaction\n",
    "poly_degree = 2  # Adjust based on your preference or use model evaluation\n",
    "poly_features = PolynomialFeatures(degree=poly_degree, include_bias=False, interaction_only=False)\n",
    "X_train_poly = poly_features.fit_transform(X_train_pca)\n",
    "X_test_poly = poly_features.transform(X_test_pca)\n",
    "\n",
    "# Build and train the neural network\n",
    "input_dim = X_train_poly.shape[1]\n",
    "model = build_neural_network(input_dim)\n",
    "\n",
    "# Train the model for 20 epochs\n",
    "model.fit(X_train_poly, target, epochs=10, batch_size=40, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test_poly).flatten()\n",
    "\n",
    "# Calculate RMSE on Training Set\n",
    "rmse_train = sqrt(mean_squared_error(target, model.predict(X_train_poly).flatten()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE on Training Set: {rmse_train}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame({'row ID': row_ids, 'price_doc': predictions})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "result_df.to_csv('predictions_42.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with predictions and 'row_ids'\n",
    "result_df = pd.DataFrame({'row ID': row_ids, 'price_doc': predictions})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "result_df.to_csv('predictions_neural_network.csv', index=False)\n",
    "\n",
    "# Print/fetch details\n",
    "print(\"Neural Network Model with 10 epochs\")\n",
    "print(f\"RMSE on Training Set: {rmse_train}\")\n",
    "print(\"Number of Features Used:\", X_train_poly.shape[1])\n",
    "print(\"Architecture Details:\")\n",
    "print(\"Dropout Rate: 0.08\")\n",
    "print(\"Batch Size: 40\")\n",
    "print(\"Number of Epochs: 10\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "train_df = pd.read_csv('train.csv')  # Update with your file path\n",
    "test_df = pd.read_csv('test.csv')  # Update with your file path\n",
    "\n",
    "# Save row IDs for the final output\n",
    "row_ids = test_df['row ID']\n",
    "\n",
    "\n",
    "# Step 1: Identify the top 100 most occurring categories\n",
    "top_categories = train_df['sub_area'].value_counts().head(100).index.tolist()\n",
    "\n",
    "# Step 2: Replace other categories with 'other'\n",
    "train_df.loc[~train_df['sub_area'].isin(top_categories), 'sub_area'] = 'other'\n",
    "test_df.loc[~test_df['sub_area'].isin(top_categories), 'sub_area'] = 'other'\n",
    "\n",
    "# Drop 'sub_area' from both datasets\n",
    "test_df = test_df.drop(columns=['row ID'])\n",
    "\n",
    "# Identify and label encode categorical columns\n",
    "# categorical_cols = train_df.select_dtypes(include='object').columns\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# for col in categorical_cols:\n",
    "#     train_df[col] = label_encoder.fit_transform(train_df[col])\n",
    "#     test_df[col] = label_encoder.transform(test_df[col])\n",
    "\n",
    "# Encode categorical variables\n",
    "train_encoded = pd.get_dummies(train_df, drop_first=True)\n",
    "test_encoded = pd.get_dummies(test_df, drop_first=True)\n",
    "\n",
    "# Separate features and target variable\n",
    "X_train = train_encoded.drop(columns=['price_doc'])\n",
    "y_train = train_encoded['price_doc']\n",
    "\n",
    "# Impute missing values with the median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "test_encoded = imputer.transform(test_encoded)\n",
    "\n",
    "# Scale features to a specific range\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "test_encoded_scaled = scaler.transform(test_encoded)\n",
    "\n",
    "# Feature selection based on variance threshold\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "X_train_var = selector.fit_transform(X_train_scaled)\n",
    "X_test_var = selector.transform(test_encoded_scaled)\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "pca = PCA(n_components=30)\n",
    "X_train_pca = pca.fit_transform(X_train_var)\n",
    "X_test_pca = pca.transform(X_test_var)\n",
    "\n",
    "\n",
    "# Create polynomial features without interaction\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "X_train_poly = poly.fit_transform(X_train_pca)\n",
    "X_test_poly = poly.transform(X_test_pca)\n",
    "\n",
    "# Build and train the neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_poly.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def build_neural_network(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_dim=input_dim, kernel_regularizer=l2(0.001)))  # Added L2 regularization\n",
    "    \n",
    "    model.add(Dropout(0.3))  # Adjusted dropout rate\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))  # Added L2 regularization\n",
    "    model.add(Dropout(0.3))  # Adjusted dropout rate\n",
    "    \n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))  # Added L2 regularization\n",
    "    model.add(Dropout(0.3))  # Adjusted dropout rate\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.0005)  # Adjusted learning rate\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['RootMeanSquaredError'])\n",
    "    return model\n",
    "\n",
    "# Add EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "model = build_neural_network(X_train_poly.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "model.fit(X_train_poly, y_train, epochs=10, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test_poly).flatten()\n",
    "# print(f\"RMSE on Training Set: {rmse_train}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with predictions and 'row_ids'\n",
    "result_df = pd.DataFrame({'row ID': row_ids, 'price_doc': predictions})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "result_df.to_csv('predictions_44.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Load the dataset\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "# Save row IDs for the final output\n",
    "row_ids = df_test['row ID']\n",
    "\n",
    "# Drop 'sub_area' from both datasets\n",
    "df_train.drop(columns=['sub_area'], inplace=True)\n",
    "df_test.drop(columns=['sub_area', 'row ID'], inplace=True)\n",
    "\n",
    "# Identify and label encode categorical columns\n",
    "categorical_columns = df_train.select_dtypes(include='object').columns\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for column in categorical_columns:\n",
    "    df_train[column] = label_encoder.fit_transform(df_train[column])\n",
    "    df_test[column] = label_encoder.transform(df_test[column])\n",
    "\n",
    "# Encode categorical variables\n",
    "df_train_encoded = pd.get_dummies(df_train, drop_first=True)\n",
    "df_test_encoded = pd.get_dummies(df_test, drop_first=True)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df_train_encoded.drop(columns=['price_doc'])\n",
    "y = df_train_encoded['price_doc']\n",
    "\n",
    "# Impute missing values with the median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "df_test_imputed = imputer.transform(df_test_encoded)\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "df_test_scaled = scaler.transform(df_test_imputed)\n",
    "\n",
    "# Feature selection based on variance threshold\n",
    "variance_threshold = 0.01\n",
    "selector = VarianceThreshold(threshold=variance_threshold)\n",
    "X_train_selected = selector.fit_transform(X_scaled)\n",
    "X_test_selected = selector.transform(df_test_scaled)\n",
    "\n",
    "# PCA for dimensionality reduction\n",
    "pca = PCA(n_components=10)\n",
    "X_train_pca = pca.fit_transform(X_train_selected)\n",
    "X_test_pca = pca.transform(X_test_selected)\n",
    "\n",
    "# Polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "X_train_poly = poly.fit_transform(X_train_pca)\n",
    "X_test_poly = poly.transform(X_test_pca)\n",
    "\n",
    "# Build and train the neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_neural_network(input_dim):\n",
    "    model = Sequential([\n",
    "        Dropout(0.08),  # Adjusted dropout rate\n",
    "        Dense(512, activation='relu'),  # Increased neurons\n",
    "        Dense(256, activation='relu', input_dim=input_dim),  # Changed neurons\n",
    "        Dense(128, activation='relu'),  # Additional layer\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='linear')  # Output layer\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=0.001)  # Adjusted learning rate\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['RootMeanSquaredError'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10210/10210 [==============================] - 18s 2ms/step - loss: 204974529183744.0000 - root_mean_squared_error: 14316932.0000 - val_loss: 166613626126336.0000 - val_root_mean_squared_error: 12907890.0000\n",
      "Epoch 2/10\n",
      "10210/10210 [==============================] - 17s 2ms/step - loss: 190007641899008.0000 - root_mean_squared_error: 13784326.0000 - val_loss: 167953119051776.0000 - val_root_mean_squared_error: 12959673.0000\n",
      "Epoch 3/10\n",
      "10210/10210 [==============================] - 19s 2ms/step - loss: 186555008286720.0000 - root_mean_squared_error: 13658514.0000 - val_loss: 167748688674816.0000 - val_root_mean_squared_error: 12951783.0000\n",
      "Epoch 4/10\n",
      "10210/10210 [==============================] - 19s 2ms/step - loss: 182958224834560.0000 - root_mean_squared_error: 13526205.0000 - val_loss: 173526594093056.0000 - val_root_mean_squared_error: 13172949.0000\n",
      "Epoch 5/10\n",
      " 2449/10210 [======>.......................] - ETA: 14s - loss: 178088973434880.0000 - root_mean_squared_error: 13344998.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yahyaahmedkhan/Downloads/iml-second-competition-regression-analysis/garbar2.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yahyaahmedkhan/Downloads/iml-second-competition-regression-analysis/garbar2.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m build_neural_network(X_train_poly\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yahyaahmedkhan/Downloads/iml-second-competition-regression-analysis/garbar2.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train_poly, y, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yahyaahmedkhan/Downloads/iml-second-competition-regression-analysis/garbar2.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Make predictions\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yahyaahmedkhan/Downloads/iml-second-competition-regression-analysis/garbar2.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test_poly)\u001b[39m.\u001b[39mflatten()\n",
      "File \u001b[0;32m~/miniconda3/envs/tfenv2/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfenv2/lib/python3.10/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1784\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/tfenv2/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfenv2/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:841\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m    840\u001b[0m   \u001b[39mif\u001b[39;00m without_tracing:\n\u001b[0;32m--> 841\u001b[0m     _frequent_tracing_detector_manager\u001b[39m.\u001b[39;49mcalled_without_tracing(\n\u001b[1;32m    842\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_key_for_call_stats)\n\u001b[1;32m    843\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    844\u001b[0m     _frequent_tracing_detector_manager\u001b[39m.\u001b[39mcalled_with_tracing(\n\u001b[1;32m    845\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_key_for_call_stats, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_python_function,\n\u001b[1;32m    846\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_omit_frequent_tracing_warning)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfenv2/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:195\u001b[0m, in \u001b[0;36m_FrequentTracingDetectorManager.called_without_tracing\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalled_without_tracing\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[1;32m    194\u001b[0m   \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m--> 195\u001b[0m     detector \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_detector(key)\n\u001b[1;32m    196\u001b[0m     detector\u001b[39m.\u001b[39mcalled_without_tracing()\n",
      "File \u001b[0;32m~/miniconda3/envs/tfenv2/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:188\u001b[0m, in \u001b[0;36m_FrequentTracingDetectorManager._get_detector\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    185\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detectors \u001b[39m=\u001b[39m weakref\u001b[39m.\u001b[39mWeakKeyDictionary()  \u001b[39m# GUARDED_BY(self._lock)\u001b[39;00m\n\u001b[1;32m    186\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock \u001b[39m=\u001b[39m threading\u001b[39m.\u001b[39mLock()\n\u001b[0;32m--> 188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_detector\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[1;32m    189\u001b[0m   \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detectors:\n\u001b[1;32m    190\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detectors[key] \u001b[39m=\u001b[39m _FrequentTracingDetector()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = build_neural_network(X_train_poly.shape[1])\n",
    "model.fit(X_train_poly, y, epochs=10, batch_size=16, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test_poly).flatten()\n",
    "\n",
    "# Calculate RMSE\n",
    "# rmse = sqrt(mean_squared_error(y, model.predict(X_train_poly).flatten()))\n",
    "\n",
    "# Output DataFrame\n",
    "# submission_df = pd.DataFrame({'row ID': row_ids, 'price_doc': predictions})\n",
    "# submission_df.to_csv('path/to/submission.csv', index=False)\n",
    "\n",
    "# # Print details\n",
    "# print(\"Neural Network Model with 50 epochs\")\n",
    "# print(f\"RMSE on Training Set: {rmse}\")\n",
    "# print(\"Number of Features Used:\", X_train_poly.shape[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
