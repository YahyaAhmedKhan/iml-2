{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_neural_network(input_dim):\n",
    "    model = Sequential([\n",
    "        Dropout(0.08),  # Adjusted dropout rate\n",
    "        Dense(512, activation='relu'),  # Increased neurons\n",
    "        Dense(256, activation='relu', input_dim=input_dim),  # Changed neurons\n",
    "        Dense(128, activation='relu'),  # Additional layer\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='linear')  # Output layer\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=0.001)  # Adjusted learning rate\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "# Save row IDs for the final output\n",
    "row_ids = df_test['row ID']\n",
    "\n",
    "# Drop 'sub_area' from both datasets\n",
    "df_train = df_train.drop(columns=['sub_area'])\n",
    "df_test = df_test.drop(columns=['sub_area', 'row ID'])\n",
    "\n",
    "# Identify and label encode categorical columns\n",
    "categorical_columns = df_train.select_dtypes(include='object').columns\n",
    "label_encoder = LabelEncoder()\n",
    "for column in categorical_columns:\n",
    "    df_train[column] = label_encoder.fit_transform(df_train[column])\n",
    "    df_test[column] = label_encoder.transform(df_test[column])\n",
    "\n",
    "# Encode categorical variables\n",
    "X_train = pd.get_dummies(df_train, drop_first=True)\n",
    "X_test = pd.get_dummies(df_test, drop_first=True)\n",
    "\n",
    "# Separate features and target variable\n",
    "features = X_train.drop(columns=['price_doc'])\n",
    "target = X_train['price_doc']\n",
    "\n",
    "# Impute missing values with the median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train = imputer.fit_transform(features)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Scale features to a specific range\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Feature selection based on variance threshold\n",
    "variance_threshold = 0.01  # Set your desired threshold\n",
    "selector = VarianceThreshold(threshold=variance_threshold)\n",
    "X_train_high_variance = selector.fit_transform(X_train_scaled)\n",
    "X_test_high_variance = selector.transform(X_test_scaled)\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "pca_components = 10 # Adjust based on your preference or use model evaluation\n",
    "pca = PCA(n_components=pca_components)\n",
    "X_train_pca = pca.fit_transform(X_train_high_variance)\n",
    "X_test_pca = pca.transform(X_test_high_variance)\n",
    "\n",
    "# Create polynomial features without interaction\n",
    "poly_degree = 2  # Adjust based on your preference or use model evaluation\n",
    "poly_features = PolynomialFeatures(degree=poly_degree, include_bias=False, interaction_only=False)\n",
    "X_train_poly = poly_features.fit_transform(X_train_pca)\n",
    "X_test_poly = poly_features.transform(X_test_pca)\n",
    "\n",
    "# Build and train the neural network\n",
    "input_dim = X_train_poly.shape[1]\n",
    "model = build_neural_network(input_dim)\n",
    "\n",
    "# Train the model for 20 epochs\n",
    "model.fit(X_train_poly, target, epochs=10, batch_size=40, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test_poly).flatten()\n",
    "\n",
    "# Calculate RMSE on Training Set\n",
    "rmse_train = sqrt(mean_squared_error(target, model.predict(X_train_poly).flatten()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE on Training Set: {rmse_train}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame({'row ID': row_ids, 'price_doc': predictions})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "result_df.to_csv('predictions_42.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with predictions and 'row_ids'\n",
    "result_df = pd.DataFrame({'row ID': row_ids, 'price_doc': predictions})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "result_df.to_csv('predictions_neural_network.csv', index=False)\n",
    "\n",
    "# Print/fetch details\n",
    "print(\"Neural Network Model with 10 epochs\")\n",
    "print(f\"RMSE on Training Set: {rmse_train}\")\n",
    "print(\"Number of Features Used:\", X_train_poly.shape[1])\n",
    "print(\"Architecture Details:\")\n",
    "print(\"Dropout Rate: 0.08\")\n",
    "print(\"Batch Size: 40\")\n",
    "print(\"Number of Epochs: 10\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "train_df = pd.read_csv('train.csv')  # Update with your file path\n",
    "test_df = pd.read_csv('test.csv')  # Update with your file path\n",
    "\n",
    "# Save row IDs for the final output\n",
    "row_ids = test_df['row ID']\n",
    "\n",
    "# Drop 'sub_area' from both datasets\n",
    "train_df = train_df.drop(columns=['sub_area'])\n",
    "test_df = test_df.drop(columns=['sub_area', 'row ID'])\n",
    "\n",
    "# Identify and label encode categorical columns\n",
    "categorical_cols = train_df.select_dtypes(include='object').columns\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    train_df[col] = label_encoder.fit_transform(train_df[col])\n",
    "    test_df[col] = label_encoder.transform(test_df[col])\n",
    "\n",
    "# Encode categorical variables\n",
    "train_encoded = pd.get_dummies(train_df, drop_first=True)\n",
    "test_encoded = pd.get_dummies(test_df, drop_first=True)\n",
    "\n",
    "# Separate features and target variable\n",
    "X_train = train_encoded.drop(columns=['price_doc'])\n",
    "y_train = train_encoded['price_doc']\n",
    "\n",
    "# Impute missing values with the median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "test_encoded = imputer.transform(test_encoded)\n",
    "\n",
    "# Scale features to a specific range\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "test_encoded_scaled = scaler.transform(test_encoded)\n",
    "\n",
    "# Feature selection based on variance threshold\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "X_train_var = selector.fit_transform(X_train_scaled)\n",
    "X_test_var = selector.transform(test_encoded_scaled)\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "pca = PCA(n_components=10)\n",
    "X_train_pca = pca.fit_transform(X_train_var)\n",
    "X_test_pca = pca.transform(X_test_var)\n",
    "\n",
    "# Create polynomial features without interaction\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False, interaction_only=False)\n",
    "X_train_poly = poly.fit_transform(X_train_pca)\n",
    "X_test_poly = poly.transform(X_test_pca)\n",
    "\n",
    "# Build and train the neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_poly.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def build_neural_network(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation='relu', input_dim=input_dim, kernel_regularizer=l2(0.001)))  # Added L2 regularization\n",
    "    \n",
    "    model.add(Dropout(0.3))  # Adjusted dropout rate\n",
    "    model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))  # Added L2 regularization\n",
    "    model.add(Dropout(0.3))  # Adjusted dropout rate\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))  # Added L2 regularization\n",
    "    model.add(Dropout(0.3))  # Adjusted dropout rate\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.0005)  # Adjusted learning rate\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['RootMeanSquaredError'])\n",
    "    return model\n",
    "\n",
    "# Add EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "model = build_neural_network(X_train_poly.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5105/5105 [==============================] - 9s 2ms/step - loss: 197459561152512.0000 - root_mean_squared_error: 14052030.0000 - val_loss: 181058909765632.0000 - val_root_mean_squared_error: 13455813.0000\n",
      "Epoch 2/20\n",
      "5105/5105 [==============================] - 7s 1ms/step - loss: 196975001600000.0000 - root_mean_squared_error: 14034778.0000 - val_loss: 180493601472512.0000 - val_root_mean_squared_error: 13434791.0000\n",
      "Epoch 3/20\n",
      "5105/5105 [==============================] - 7s 1ms/step - loss: 196180533313536.0000 - root_mean_squared_error: 14006446.0000 - val_loss: 182704754655232.0000 - val_root_mean_squared_error: 13516832.0000\n",
      "Epoch 4/20\n",
      "5105/5105 [==============================] - 8s 1ms/step - loss: 195275939381248.0000 - root_mean_squared_error: 13974117.0000 - val_loss: 180957793484800.0000 - val_root_mean_squared_error: 13452055.0000\n",
      "Epoch 5/20\n",
      "5105/5105 [==============================] - 7s 1ms/step - loss: 195198059544576.0000 - root_mean_squared_error: 13971330.0000 - val_loss: 179722185080832.0000 - val_root_mean_squared_error: 13406050.0000\n",
      "Epoch 6/20\n",
      "5105/5105 [==============================] - 7s 1ms/step - loss: 194717056761856.0000 - root_mean_squared_error: 13954105.0000 - val_loss: 178950265372672.0000 - val_root_mean_squared_error: 13377229.0000\n",
      "Epoch 7/20\n",
      "5105/5105 [==============================] - 7s 1ms/step - loss: 195134541004800.0000 - root_mean_squared_error: 13969057.0000 - val_loss: 178651362492416.0000 - val_root_mean_squared_error: 13366053.0000\n",
      "Epoch 8/20\n",
      "5105/5105 [==============================] - 7s 1ms/step - loss: 194748010725376.0000 - root_mean_squared_error: 13955214.0000 - val_loss: 178523620769792.0000 - val_root_mean_squared_error: 13361273.0000\n",
      "Epoch 9/20\n",
      "5105/5105 [==============================] - 7s 1ms/step - loss: 194325476540416.0000 - root_mean_squared_error: 13940067.0000 - val_loss: 178670371078144.0000 - val_root_mean_squared_error: 13366764.0000\n",
      "Epoch 10/20\n",
      "5105/5105 [==============================] - 8s 2ms/step - loss: 193920088670208.0000 - root_mean_squared_error: 13925519.0000 - val_loss: 178260990230528.0000 - val_root_mean_squared_error: 13351442.0000\n",
      "Epoch 11/20\n",
      "5105/5105 [==============================] - 8s 2ms/step - loss: 193773069926400.0000 - root_mean_squared_error: 13920240.0000 - val_loss: 178468776050688.0000 - val_root_mean_squared_error: 13359221.0000\n",
      "Epoch 12/20\n",
      "5105/5105 [==============================] - 7s 1ms/step - loss: 193417023848448.0000 - root_mean_squared_error: 13907445.0000 - val_loss: 177612886376448.0000 - val_root_mean_squared_error: 13327148.0000\n",
      "Epoch 13/20\n",
      "5105/5105 [==============================] - 8s 1ms/step - loss: 193682305187840.0000 - root_mean_squared_error: 13916979.0000 - val_loss: 177667076784128.0000 - val_root_mean_squared_error: 13329181.0000\n",
      "Epoch 14/20\n",
      "5105/5105 [==============================] - 8s 1ms/step - loss: 191750375211008.0000 - root_mean_squared_error: 13847396.0000 - val_loss: 176876735692800.0000 - val_root_mean_squared_error: 13299501.0000\n",
      "Epoch 15/20\n",
      "5105/5105 [==============================] - 8s 1ms/step - loss: 192207655010304.0000 - root_mean_squared_error: 13863898.0000 - val_loss: 176645612765184.0000 - val_root_mean_squared_error: 13290809.0000\n",
      "Epoch 16/20\n",
      "5105/5105 [==============================] - 7s 1ms/step - loss: 191772839903232.0000 - root_mean_squared_error: 13848207.0000 - val_loss: 176662188654592.0000 - val_root_mean_squared_error: 13291433.0000\n",
      "Epoch 17/20\n",
      "5105/5105 [==============================] - 8s 1ms/step - loss: 190970805092352.0000 - root_mean_squared_error: 13819219.0000 - val_loss: 175780797612032.0000 - val_root_mean_squared_error: 13258235.0000\n",
      "Epoch 18/20\n",
      "5105/5105 [==============================] - 7s 1ms/step - loss: 190953222569984.0000 - root_mean_squared_error: 13818583.0000 - val_loss: 175850523721728.0000 - val_root_mean_squared_error: 13260864.0000\n",
      "Epoch 19/20\n",
      "5105/5105 [==============================] - 7s 1ms/step - loss: 190486430089216.0000 - root_mean_squared_error: 13801682.0000 - val_loss: 175668457373696.0000 - val_root_mean_squared_error: 13253998.0000\n",
      "Epoch 20/20\n",
      "5105/5105 [==============================] - 7s 1ms/step - loss: 190196821786624.0000 - root_mean_squared_error: 13791186.0000 - val_loss: 174945040596992.0000 - val_root_mean_squared_error: 13226679.0000\n",
      "2431/2431 [==============================] - 1s 563us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "model.fit(X_train_poly, y_train, epochs=20, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test_poly).flatten()\n",
    "# print(f\"RMSE on Training Set: {rmse_train}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print RMSE on Training Set\n",
    "print(f\"RMSE on Training Set: {sqrt(mean_squared_error(y_train, model.predict(X_train_poly).flatten()))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with predictions and 'row_ids'\n",
    "result_df = pd.DataFrame({'row ID': row_ids, 'price_doc': predictions})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "result_df.to_csv('predictions_44.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
