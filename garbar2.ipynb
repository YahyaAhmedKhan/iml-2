{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_neural_network(input_dim):\n",
    "    model = Sequential([\n",
    "        Dropout(0.08),  # Adjusted dropout rate\n",
    "        Dense(512, activation='relu'),  # Increased neurons\n",
    "        Dense(256, activation='relu', input_dim=input_dim),  # Changed neurons\n",
    "        Dense(128, activation='relu'),  # Additional layer\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='linear')  # Output layer\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=0.001)  # Adjusted learning rate\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "# Save row IDs for the final output\n",
    "row_ids = df_test['row ID']\n",
    "\n",
    "# Drop 'sub_area' from both datasets\n",
    "df_train = df_train.drop(columns=['sub_area'])\n",
    "df_test = df_test.drop(columns=['sub_area', 'row ID'])\n",
    "\n",
    "# Identify and label encode categorical columns\n",
    "categorical_columns = df_train.select_dtypes(include='object').columns\n",
    "label_encoder = LabelEncoder()\n",
    "for column in categorical_columns:\n",
    "    df_train[column] = label_encoder.fit_transform(df_train[column])\n",
    "    df_test[column] = label_encoder.transform(df_test[column])\n",
    "\n",
    "# Encode categorical variables\n",
    "X_train = pd.get_dummies(df_train, drop_first=True)\n",
    "X_test = pd.get_dummies(df_test, drop_first=True)\n",
    "\n",
    "# Separate features and target variable\n",
    "features = X_train.drop(columns=['price_doc'])\n",
    "target = X_train['price_doc']\n",
    "\n",
    "# Impute missing values with the median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train = imputer.fit_transform(features)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Scale features to a specific range\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Feature selection based on variance threshold\n",
    "variance_threshold = 0.01  # Set your desired threshold\n",
    "selector = VarianceThreshold(threshold=variance_threshold)\n",
    "X_train_high_variance = selector.fit_transform(X_train_scaled)\n",
    "X_test_high_variance = selector.transform(X_test_scaled)\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "pca_components = 10 # Adjust based on your preference or use model evaluation\n",
    "pca = PCA(n_components=pca_components)\n",
    "X_train_pca = pca.fit_transform(X_train_high_variance)\n",
    "X_test_pca = pca.transform(X_test_high_variance)\n",
    "\n",
    "# Create polynomial features without interaction\n",
    "poly_degree = 2  # Adjust based on your preference or use model evaluation\n",
    "poly_features = PolynomialFeatures(degree=poly_degree, include_bias=False, interaction_only=False)\n",
    "X_train_poly = poly_features.fit_transform(X_train_pca)\n",
    "X_test_poly = poly_features.transform(X_test_pca)\n",
    "\n",
    "# Build and train the neural network\n",
    "input_dim = X_train_poly.shape[1]\n",
    "model = build_neural_network(input_dim)\n",
    "\n",
    "# Train the model for 20 epochs\n",
    "model.fit(X_train_poly, target, epochs=10, batch_size=40, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test_poly).flatten()\n",
    "\n",
    "# Calculate RMSE on Training Set\n",
    "rmse_train = sqrt(mean_squared_error(target, model.predict(X_train_poly).flatten()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE on Training Set: {rmse_train}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame({'row ID': row_ids, 'price_doc': predictions})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "result_df.to_csv('predictions_42.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with predictions and 'row_ids'\n",
    "result_df = pd.DataFrame({'row ID': row_ids, 'price_doc': predictions})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "result_df.to_csv('predictions_neural_network.csv', index=False)\n",
    "\n",
    "# Print/fetch details\n",
    "print(\"Neural Network Model with 10 epochs\")\n",
    "print(f\"RMSE on Training Set: {rmse_train}\")\n",
    "print(\"Number of Features Used:\", X_train_poly.shape[1])\n",
    "print(\"Architecture Details:\")\n",
    "print(\"Dropout Rate: 0.08\")\n",
    "print(\"Batch Size: 40\")\n",
    "print(\"Number of Epochs: 10\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "train_df = pd.read_csv('train.csv')  # Update with your file path\n",
    "test_df = pd.read_csv('test.csv')  # Update with your file path\n",
    "\n",
    "# Save row IDs for the final output\n",
    "row_ids = test_df['row ID']\n",
    "\n",
    "\n",
    "# Step 1: Identify the top 100 most occurring categories\n",
    "top_categories = train_df['sub_area'].value_counts().head(100).index.tolist()\n",
    "\n",
    "# Step 2: Replace other categories with 'other'\n",
    "train_df.loc[~train_df['sub_area'].isin(top_categories), 'sub_area'] = 'other'\n",
    "test_df.loc[~test_df['sub_area'].isin(top_categories), 'sub_area'] = 'other'\n",
    "\n",
    "# Drop 'sub_area' from both datasets\n",
    "test_df = test_df.drop(columns=['row ID'])\n",
    "\n",
    "# Identify and label encode categorical columns\n",
    "# categorical_cols = train_df.select_dtypes(include='object').columns\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# for col in categorical_cols:\n",
    "#     train_df[col] = label_encoder.fit_transform(train_df[col])\n",
    "#     test_df[col] = label_encoder.transform(test_df[col])\n",
    "\n",
    "# Encode categorical variables\n",
    "train_encoded = pd.get_dummies(train_df, drop_first=True)\n",
    "test_encoded = pd.get_dummies(test_df, drop_first=True)\n",
    "\n",
    "# Separate features and target variable\n",
    "X_train = train_encoded.drop(columns=['price_doc'])\n",
    "y_train = train_encoded['price_doc']\n",
    "\n",
    "# Impute missing values with the median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "test_encoded = imputer.transform(test_encoded)\n",
    "\n",
    "# Scale features to a specific range\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "test_encoded_scaled = scaler.transform(test_encoded)\n",
    "\n",
    "# Feature selection based on variance threshold\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "X_train_var = selector.fit_transform(X_train_scaled)\n",
    "X_test_var = selector.transform(test_encoded_scaled)\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "pca = PCA(n_components=30)\n",
    "X_train_pca = pca.fit_transform(X_train_var)\n",
    "X_test_pca = pca.transform(X_test_var)\n",
    "\n",
    "\n",
    "# Create polynomial features without interaction\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "X_train_poly = poly.fit_transform(X_train_pca)\n",
    "X_test_poly = poly.transform(X_test_pca)\n",
    "\n",
    "# Build and train the neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_poly.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def build_neural_network(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_dim=input_dim, kernel_regularizer=l2(0.001)))  # Added L2 regularization\n",
    "    \n",
    "    model.add(Dropout(0.3))  # Adjusted dropout rate\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))  # Added L2 regularization\n",
    "    model.add(Dropout(0.3))  # Adjusted dropout rate\n",
    "    \n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))  # Added L2 regularization\n",
    "    model.add(Dropout(0.3))  # Adjusted dropout rate\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.0005)  # Adjusted learning rate\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['RootMeanSquaredError'])\n",
    "    return model\n",
    "\n",
    "# Add EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "model = build_neural_network(X_train_poly.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "model.fit(X_train_poly, y_train, epochs=10, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test_poly).flatten()\n",
    "# print(f\"RMSE on Training Set: {rmse_train}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with predictions and 'row_ids'\n",
    "result_df = pd.DataFrame({'row ID': row_ids, 'price_doc': predictions})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "result_df.to_csv('predictions_44.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Load the dataset\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "# Save row IDs for the final output\n",
    "row_ids = df_test['row ID']\n",
    "\n",
    "# Drop 'sub_area' from both datasets\n",
    "df_train.drop(columns=['sub_area'], inplace=True)\n",
    "df_test.drop(columns=['sub_area', 'row ID'], inplace=True)\n",
    "\n",
    "# Identify and label encode categorical columns\n",
    "categorical_columns = df_train.select_dtypes(include='object').columns\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for column in categorical_columns:\n",
    "    df_train[column] = label_encoder.fit_transform(df_train[column])\n",
    "    df_test[column] = label_encoder.transform(df_test[column])\n",
    "\n",
    "# Encode categorical variables\n",
    "df_train_encoded = pd.get_dummies(df_train, drop_first=True)\n",
    "df_test_encoded = pd.get_dummies(df_test, drop_first=True)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df_train_encoded.drop(columns=['price_doc'])\n",
    "y = df_train_encoded['price_doc']\n",
    "\n",
    "# Impute missing values with the median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "df_test_imputed = imputer.transform(df_test_encoded)\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "df_test_scaled = scaler.transform(df_test_imputed)\n",
    "\n",
    "# Feature selection based on variance threshold\n",
    "variance_threshold = 0.01\n",
    "selector = VarianceThreshold(threshold=variance_threshold)\n",
    "X_train_selected = selector.fit_transform(X_scaled)\n",
    "X_test_selected = selector.transform(df_test_scaled)\n",
    "\n",
    "# PCA for dimensionality reduction\n",
    "pca = PCA(n_components=10)\n",
    "X_train_pca = pca.fit_transform(X_train_selected)\n",
    "X_test_pca = pca.transform(X_test_selected)\n",
    "\n",
    "# Polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "X_train_poly = poly.fit_transform(X_train_pca)\n",
    "X_test_poly = poly.transform(X_test_pca)\n",
    "\n",
    "# Build and train the neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5105/5105 [==============================] - 7s 1ms/step - loss: 219573290795008.0000 - root_mean_squared_error: 14818006.0000 - val_loss: 167209821274112.0000 - val_root_mean_squared_error: 12930964.0000\n",
      "Epoch 2/10\n",
      "5105/5105 [==============================] - 5s 999us/step - loss: 190693846810624.0000 - root_mean_squared_error: 13809194.0000 - val_loss: 165624475025408.0000 - val_root_mean_squared_error: 12869517.0000\n",
      "Epoch 3/10\n",
      "5105/5105 [==============================] - 5s 1ms/step - loss: 189267968000000.0000 - root_mean_squared_error: 13757470.0000 - val_loss: 169372689629184.0000 - val_root_mean_squared_error: 13014326.0000\n",
      "Epoch 4/10\n",
      "5105/5105 [==============================] - 5s 1ms/step - loss: 187687671693312.0000 - root_mean_squared_error: 13699915.0000 - val_loss: 165543239745536.0000 - val_root_mean_squared_error: 12866361.0000\n",
      "Epoch 5/10\n",
      "5105/5105 [==============================] - 5s 1ms/step - loss: 184439669784576.0000 - root_mean_squared_error: 13580857.0000 - val_loss: 166128697475072.0000 - val_root_mean_squared_error: 12889092.0000\n",
      "Epoch 6/10\n",
      "5105/5105 [==============================] - 6s 1ms/step - loss: 182408133476352.0000 - root_mean_squared_error: 13505856.0000 - val_loss: 166707746308096.0000 - val_root_mean_squared_error: 12911535.0000\n",
      "Epoch 7/10\n",
      "5105/5105 [==============================] - 5s 1ms/step - loss: 181712617209856.0000 - root_mean_squared_error: 13480082.0000 - val_loss: 165737234694144.0000 - val_root_mean_squared_error: 12873897.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x150df9ea0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# def build_neural_network(input_dim):\n",
    "#     model = Sequential([\n",
    "#         Dropout(0.08),  # Adjusted dropout rate\n",
    "#         Dense(512, activation='relu'),  # Increased neurons\n",
    "#         Dense(256, activation='relu', input_dim=input_dim),  # Changed neurons\n",
    "#         Dense(128, activation='relu'),  # Additional layer\n",
    "#         Dense(64, activation='relu'),\n",
    "#         Dense(1, activation='linear')  # Output layer\n",
    "#     ])\n",
    "#     optimizer = Adam(learning_rate=0.001)  # Adjusted learning rate\n",
    "#     model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['RootMeanSquaredError'])\n",
    "#     return model\n",
    "\n",
    "# model = build_neural_network(X_train_poly.shape[1])\n",
    "# model.fit(X_train_poly, y, epochs=10, batch_size=16, validation_split=0.1, verbose=1)\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "\n",
    "def build_neural_network(input_dim):\n",
    "    model = Sequential([\n",
    "        Dropout(0.08),  # Adjusted dropout rate\n",
    "        # Dense(512, activation='relu', kernel_regularizer=l2(0.01)),  # Added L2 regularization\n",
    "        \n",
    "        Dense(256, activation='relu', input_dim=input_dim, kernel_regularizer=l2(0.01)),  # Added L2 regularization\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.01)),  # Added L2 regularization\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.01)),  # Added L2 regularization\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.01)),  # Added L2 regularization\n",
    "        Dense(16, activation='relu', kernel_regularizer=l2(0.01)),  # Added L2 regularization\n",
    "        Dense(1, activation='linear')  # Output layer\n",
    "    ])\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)  # Adjusted learning rate\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['RootMeanSquaredError'])\n",
    "    return model\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "model = build_neural_network(X_train_poly.shape[1])\n",
    "model.fit(X_train_poly, y, epochs=10, batch_size=64, validation_split=0.1, verbose=1, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2431/2431 [==============================] - 2s 614us/step\n"
     ]
    }
   ],
   "source": [
    "# model = build_neural_network(X_train_poly.shape[1])\n",
    "# model.fit(X_train_poly, y, epochs=10, batch_size=16, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test_poly).flatten()\n",
    "\n",
    "# Calculate RMSE\n",
    "# rmse = sqrt(mean_squared_error(y, model.predict(X_train_poly).flatten()))\n",
    "\n",
    "# Output DataFrame\n",
    "# submission_df = pd.DataFrame({'row ID': row_ids, 'price_doc': predictions})\n",
    "# submission_df.to_csv('path/to/submission.csv', index=False)\n",
    "\n",
    "# # Print details\n",
    "# print(\"Neural Network Model with 50 epochs\")\n",
    "# print(f\"RMSE on Training Set: {rmse}\")\n",
    "# print(\"Number of Features Used:\", X_train_poly.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with predictions and 'row_ids'\n",
    "result_df = pd.DataFrame({'row ID': row_ids, 'price_doc': predictions})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "result_df.to_csv('predictions_46.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
